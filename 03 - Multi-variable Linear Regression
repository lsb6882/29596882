
Cost function in TensorFlow
X = np.array([1, 2, 3]) # 입력값 = 1, 2, 3
Y = np.array([1, 2, 3]) # 출력값 = 1, 2, 3

def cost_func(W, X, Y): # X, Y에대해 W값이 주어졌을때 Cost를 계산하는 함수
  hypothesis = X * W 
  return tf.reduce_mean(tf.square(hypothesis - Y)) # (가설값 - 실제값) 평균의 제곱

W_values = np.linspace(-3, 5, num=15) # -3에서 5사이를 15개의 구간으로 나눠서 값을가짐
cost_values = []

for feed_W in W_values:
    curr_cost = cost_func(feed_W, X, Y)  # W값에 따른 Cost변화를 출력
    cost_values.append(curr_cost) 
    print("{:6.3f} | {:10.5f}".format(feed_W, curr_cost))
    -3.000 |   74.66667
-2.429 |   54.85714
-1.857 |   38.09524
-1.286 |   24.38095
-0.714 |   13.71429
-0.143 |    6.09524
 0.429 |    1.52381
 1.000 |    0.00000
 1.571 |    1.52381
 2.143 |    6.09524
 2.714 |   13.71429
 3.286 |   24.38095
 3.857 |   38.09524
 4.429 |   54.85714
 5.000 |   74.66667
 
 Gradient descent 구현
 tf.set_random_seed(0)  # for reproducibility #다음에 코드를 다시 수행 했을때도 동일하게 재연될 수 있기 위해 랜덤시드 초기화
x_data = [1., 2., 3., 4.] # 입력값= 1, 2, 3, 4
y_data = [1., 3., 5., 7.] # 출력값= 1, 3, 5, 7

W = tf.Variable(tf.random_normal([1], -100., 100.)) #정규분포를 따르는 랜덤넘버를 1개짜리로 변수를 만들어서 w에할당해서 정의를한다

for step in range(300): # 300회 수행
    hypothesis = W * X # hypothesis 함수를 W * X로정의
    cost = tf.reduce_mean(tf.square(hypothesis - Y)) #차이제곱의평균 

    alpha = 0.01
    gradient = tf.reduce_mean(tf.multiply(tf.multiply(W, X) - Y, X)) # wx -Y *X 의 평균을 구한다
    descent = W - tf.multiply(alpha, gradient)    #gradient에 alpha를 곱한 값이 새로운 W 값
    W.assign(descent) #새로운 W값을 업데이트한다.
    
    if step % 10 == 0:   # 300회를 수행하며 10번에 1번씩 cost값과 w값 출력
        print('{:5} | {:10.4f} | {:10.6f}'.format( 
            step, cost.numpy(), W.numpy()[0]))
