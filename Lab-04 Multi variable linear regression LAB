# data and label
x1 = [ 73.,  93.,  89.,  96.,  73.]   #입력값, Y=정답
x2 = [ 80.,  88.,  91.,  98.,  66.]
x3 = [ 75.,  93.,  90., 100.,  70.]
Y  = [152., 185., 180., 196., 142.]

# random weights
w1 = tf.Variable(tf.random_normal([1])) #변수가 3개이므로 W도 3개가 필요하다.
w2 = tf.Variable(tf.random_normal([1])) #b는 1개필요하다.
w3 = tf.Variable(tf.random_normal([1])) #초기값을 1로 지정했다.(랜덤값)
b  = tf.Variable(tf.random_normal([1]))

learning_rate = 0.000001 #작은값으로 설정

for i in range(1000+1): #1001번 수행
    # tf.GradientTape() to record the gradient of the cost function #GradientTape을 사용하여 구현함.
    with tf.GradientTape() as tape:                     # 변수들의 정보를 Tape에 기록 
        hypothesis = w1 * x1 +  w2 * x2 + w3 * x3 + b
        cost = tf.reduce_mean(tf.square(hypothesis - Y)) # 가설값에서 정답을 뺀 값 의 제곱의 평균값
    # calculates the gradients of the cost
    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b]) #Tape의 Gradient를 호출하여 변수들에 대한 기울기 값을 구함.
    
    # update w1,w2,w3 and b
    w1.assign_sub(learning_rate * w1_grad) #Grad 값에 learing_rate 값을 곱해서 뺀 뒤 할당해줌
    w2.assign_sub(learning_rate * w2_grad) #각 각 업데이트 
    w3.assign_sub(learning_rate * w3_grad)
    b.assign_sub(learning_rate * b_grad)

    if i % 50 == 0:          #50번마다 출력한다.
      print("{:5} | {:12.4f}".format(i, cost.numpy()))
       0 |   11325.9121
   50 |     135.3618
  100 |      11.1817  #처음에는 매우 큰 값이 줄어들다가 점점 거의 줄어들지않음
  150 |       9.7940
  200 |       9.7687
  250 |       9.7587
  300 |       9.7489
  350 |       9.7389
  400 |       9.7292
  450 |       9.7194
  500 |       9.7096
  550 |       9.6999
  600 |       9.6903
  650 |       9.6806
  700 |       9.6709
  750 |       9.6612
  800 |       9.6517
  850 |       9.6421
  900 |       9.6325
  950 |       9.6229
 1000 |       9.6134

Multi-variable linear regression (2)
Matrix 사용
data = np.array([
    # X1,   X2,    X3,   y
    [ 73.,  80.,  75., 152. ],  
    [ 93.,  88.,  93., 185. ],
    [ 89.,  91.,  90., 180. ],
    [ 96.,  98., 100., 196. ],
    [ 73.,  66.,  70., 142. ]
], dtype=np.float32)

# slice data
X = data[:, :-1]  # :=행 :-1 =컬럼 #5행 3열 입력데이터
y = data[:, [-1]]  # 5행 1열 출력데이터

W = tf.Variable(tf.random_normal([3, 1])) #변수 즉 컬럼이 3개임으로 row가 1개가 필요, 출력값이 1개니까 weight의 컬럼도 1개
b = tf.Variable(tf.random_normal([1])) # b는 1.

learning_rate = 0.000001 #learning_rate는 작은 상수값으로 설정.

# hypothesis, prediction function
def predict(X):
    return tf.matmul(X, W) + b  #predict함수는 X W로 정의, b는 생략가능.

print("epoch | cost")

n_epochs = 2000  #2000번의 순회
for i in range(n_epochs+1): #2001회
    # tf.GradientTape() to record the gradient of the cost function
    with tf.GradientTape() as tape:
        cost = tf.reduce_mean((tf.square(predict(X) - y)))

    # calculates the gradients of the loss
    W_grad, b_grad = tape.gradient(cost, [W, b])

    # updates parameters (W and b)
    
    W.assign_sub(learning_rate * W_grad) # W 와 b 값을 지속적으로 업데이트
    b.assign_sub(learning_rate * b_grad)
    
    if i % 100 == 0: #100번마다 출력.
        print("{:5} | {:10.4f}".format(i, cost.numpy()))
        epoch | cost
    0 |  5455.5903
  100 |    31.7443
  200 |    30.9326
  300 |    30.7894
  400 |    30.6468             #처음에는 급격히 줄다가 점차 거의 줄어들지 않음, 최적화가 빠름.
  500 |    30.5055
  600 |    30.3644
  700 |    30.2242
  800 |    30.0849
  900 |    29.9463
 1000 |    29.8081
 1100 |    29.6710
 1200 |    29.5348
 1300 |    29.3989
 1400 |    29.2641
 1500 |    29.1299
 1600 |    28.9961
 1700 |    28.8634
 1800 |    28.7313
 1900 |    28.5997
 2000 |    28.4689       
